{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ba4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGED DATA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error # for calculating the cost function\n",
    "from sklearn.ensemble import RandomForestRegressor # for building the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "data = pd.read_csv('/Users/selinzobu/Desktop/TILES/111OMFITBASES111.csv', parse_dates = ['Timestamp'])\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "#fitbit[\"Timestamp\"] = pd.to_datetime(fitbit[\"Timestamp\"], utc=True)\n",
    "#print(data.columns)\n",
    "\n",
    "bases = pd.read_csv('/Users/selinzobu/Desktop/TILES/1BASESTRESSD.csv', parse_dates = ['Timestamp'])\n",
    "bases = bases.loc[:, ~bases.columns.str.contains('^Unnamed')]\n",
    "\n",
    "FOM = pd.read_csv('/Users/selinzobu/Desktop/TILES/1FITOMSTRESSD.csv', parse_dates = ['Timestamp'])\n",
    "FOM = FOM.loc[:, ~FOM.columns.str.contains('^Unnamed')]\n",
    "\n",
    "X = data.iloc[:,2:65]  #independent columns\n",
    "X = X.drop('days', axis = 1)\n",
    "y = data.iloc[:,-1]    #target column \n",
    "\n",
    "X_base = bases.drop(['participant_id', 'Timestamp', 'mstressd'], axis = 1)  #independent columns\n",
    "y_base = bases['mstressd']    #target column \n",
    "\n",
    "X_FOM = FOM.drop(['participant_id', 'Timestamp', 'mstressd', 'days'], axis = 1)  #independent columns\n",
    "y_FOM = FOM['mstressd']   #target column \n",
    "\n",
    "#for col in X.columns:\n",
    " #   X[col] = X[col].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b63855",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X[['AvgHeartRateS', 'RMSStdDev_msM', 'AvgBreathingRateM', 'AvgZAccel_gM', 'StdDevBreathingRateM', 'Fat Burn_caloriesOut', 'NumberSteps', 'iod_ID', 'iod_OD', 'ocb', 'pan_NegAffect', 'stai', 'rand_EmotionalWellbeing', 'swls', 'pss', 'mpfi_Flexibility_Defusion', 'pcq_Hope', 'pcq_Optimism', 'chss_ChallengeStressors', 'chss_HindranceStressors']]\n",
    "\n",
    "X2_FOM = X_FOM[['Cardio_caloriesOut', 'AvgHeartRateS', 'RMSStdDev_msM', 'AvgBreathingRateM', 'AvgYAccel_gM', 'AvgZAccel_gM', 'StdDevBreathingRateM', 'StdDevBreathingRateS', 'StdDevGForceM', 'StdDevYAccel_gM']]\n",
    "\n",
    "X2_base = X_base[['rand_EmotionalWellbeing', 'ocb', 'pcq_Hope', 'mpfi_Flexibility_Defusion', 'mpfi_Flexibility_SelfAsContext', 'iod_ID', 'mpfi_Flexibility_PresentMomentAwareness', 'swls', 'chss_ChallengeStressors', 'stai', 'pan_NegAffect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4331229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2989, 20) (3126, 10) (7868, 11) (2989,)\n"
     ]
    }
   ],
   "source": [
    "print(X2.shape, X2_FOM.shape, X2_base.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105a5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y_train, y_test = train_test_split(\n",
    "    X2, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X2b_train, X2b_test, yb_train, yb_test = train_test_split(\n",
    "    X2_base, y_base, test_size=0.3, random_state=42)\n",
    "\n",
    "X2FOM_train, X2FOM_test, yFOM_train, yFOM_test = train_test_split(\n",
    "    X2_FOM, y_FOM, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e73752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2_train: (2092, 20) X2_test: (897, 20) y_train: (2092,) y_test: (897,)\n",
      "X2b_train: (5507, 11) X2b_test: (2361, 11) yb_train: (5507,) yb_test: (2361,)\n",
      "X2FOM_train: (2188, 10) X2FOM_test: (938, 10) yFOM_train: (2188,) yFOM_test: (938,)\n"
     ]
    }
   ],
   "source": [
    "print('X2_train:', X2_train.shape, 'X2_test:', X2_test.shape, 'y_train:', y_train.shape,'y_test:', y_test.shape)\n",
    "#print(y_test.head())\n",
    "print('X2b_train:', X2b_train.shape, 'X2b_test:', X2b_test.shape, 'yb_train:', yb_train.shape,'yb_test:', yb_test.shape)\n",
    "#print(yb_test.head())\n",
    "print('X2FOM_train:', X2FOM_train.shape, 'X2FOM_test:', X2FOM_test.shape, 'yFOM_train:', yFOM_train.shape,'yFOM_test:', yFOM_test.shape)\n",
    "#print(yFOM_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8970a997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.902963671128107 1.803068821499909 1.9079067641681902\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean(), yb_train.mean(), yFOM_train.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43469847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average baseline error merged:  0.75\n",
      "Average baseline error baseline:  0.75\n",
      "Average baseline error FOM:  0.73\n",
      "897 2361 938\n"
     ]
    }
   ],
   "source": [
    "#The baseline prediction for our case can be the historical stress score averages. \n",
    "#In other words, our baseline is the error we would get if we simply predicted the average\n",
    "#stress score for all days.\n",
    "\n",
    "# The baseline predictions are the historical averages\n",
    "Mbaseline_preds = y_train.mean()\n",
    "Bbaseline_preds = yb_train.mean()\n",
    "FOMbaseline_preds = yFOM_train.mean()\n",
    "\n",
    "# Baseline errors, and display average baseline error\n",
    "Mbaseline_errors = abs(Mbaseline_preds - y_test)\n",
    "Bbaseline_errors = abs(Bbaseline_preds - yb_test)\n",
    "FOMbaseline_errors = abs(FOMbaseline_preds - yFOM_test)\n",
    "\n",
    "print('Average baseline error merged: ', round(np.mean(Mbaseline_errors), 2))\n",
    "print('Average baseline error baseline: ', round(np.mean(Bbaseline_errors), 2))\n",
    "print('Average baseline error FOM: ', round(np.mean(FOMbaseline_errors), 2))\n",
    "\n",
    "print(len(Mbaseline_errors), len(Bbaseline_errors), len(FOMbaseline_errors))\n",
    "Mbaseline_errors = list(Mbaseline_errors)\n",
    "Bbaseline_errors = list(Bbaseline_errors)\n",
    "FOMbaseline_errors = list(FOMbaseline_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91052c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('normalize', StandardScaler()), ('model', SVR())])\n",
      "TransformedTargetRegressor(regressor=Pipeline(steps=[('normalize',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('model', SVR())]),\n",
      "                           transformer=StandardScaler())\n",
      "Mean MAE: 0.571\n",
      "\n",
      "RMSE:  0.662 \n",
      "MAE:  0.45737230794465056 \n",
      "MSE:  0.4386940002204391\n"
     ]
    }
   ],
   "source": [
    "#TRAINING SCORE\n",
    "\n",
    "#https://machinelearningmastery.com/how-to-transform-target-variables-for-regression-with-scikit-learn/\n",
    "\n",
    "#X2_train, ny_train >> not scaled\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import absolute\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error # for calculating the cost function\n",
    "\n",
    "\n",
    "# prepare the model with input scaling\n",
    "pipeline = Pipeline(steps=[('normalize', StandardScaler()), ('model', SVR(kernel = 'rbf'))])\n",
    "print(pipeline)\n",
    "# prepare the model with target scaling\n",
    "model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "print(model)\n",
    "# evaluate model\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "#Used not scaled values as they will be scaled here\n",
    "scorestrain = cross_val_score(model, X2_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "scorestest = cross_val_score(model, X2_test, y_test, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "model.fit(X2_test, y_test)\n",
    "y_pred = model.predict(X2_test)\n",
    "# convert scores to positive\n",
    "scorestrain = absolute(scorestrain)\n",
    "rmse = float(format(np.sqrt(mean_squared_error(y_test, y_pred)), '.3f'))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# summarize the results TRAINING\n",
    "s_mean = mean(scorestrain)\n",
    "print('Mean MAE: %.3f' % (s_mean)) #Mean MAE: 0.571 > c1 gamma1 Mean MAE: 0.625\n",
    "print(\"\\nRMSE: \", rmse, '\\nMAE: ', mae, '\\nMSE: ', mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529dc831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from seaborn import load_dataset, pairplot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc9a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092,) <class 'pandas.core.series.Series'>\n",
      "(2092, 1) <class 'numpy.ndarray'>\n",
      "(897, 1) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#StandardScaler, the class we use to scale the data, takes in a 2D array; otherwise, \n",
    "#it returns an error.\n",
    "\n",
    "print(y_train.shape, type(y_train))\n",
    "ny_train = y_train.to_numpy()\n",
    "ny_test = y_test.to_numpy()\n",
    "\n",
    "\n",
    "ny_train = (ny_train).reshape(-1,1)\n",
    "ny_test = (ny_test).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(ny_train.shape, type(ny_train))\n",
    "print(ny_test.shape, type(ny_test))\n",
    "\n",
    "\n",
    "StdS_Xt = StandardScaler()\n",
    "StdS_yt = StandardScaler()\n",
    "StdS_Xte = StandardScaler()\n",
    "StdS_yte = StandardScaler()\n",
    "\n",
    "XT = StdS_Xt.fit_transform(X2_train)\n",
    "yT = StdS_yt.fit_transform(ny_train)\n",
    "XTe = StdS_Xte.fit_transform(X2_test)\n",
    "yTe = StdS_yte.fit_transform(ny_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ee3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALING TRAINING\n",
    "\n",
    "#print(\"Scaled X_l:\", XT)\n",
    "#print(\"Scaled y_p:\", yT)\n",
    "#print(yT.ravel().shape,yT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c4b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.74 degrees.\n"
     ]
    }
   ],
   "source": [
    "# import the model\n",
    "#XT = StdS_Xt.fit_transform(X2_train)\n",
    "#yT = StdS_yt.fit_transform(ny_train)\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "# create the model object\n",
    "regressor = SVR(kernel = 'rbf')\n",
    "# fit the model on the data\n",
    "regressor.fit(X2_train, ny_train.ravel()) #not scaled\n",
    " \n",
    "# Predicting the target values of the test set\n",
    "y2_pred = regressor.predict(X2_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y2_pred - ny_test.ravel())\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "\n",
    "#0.64 degrees TEST SCORE default\n",
    "#Mean Absolute Error: 0.73 degrees. C =1, gamma =1\n",
    "#Mean Absolute Error: 0.74 degrees. no scaling default\n",
    "#Mean Absolute Error: 0.76 degrees. no scaling C = 1, gamma =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4af4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS VALIDATION - PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1970745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XT = StdS_Xt.fit_transform(X2_train) x_train\n",
    "#yT = StdS_yt.fit_transform(ny_train) y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4db4930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING SCORES\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def tune_rbfsvm(x_T, y_T):\n",
    "        \n",
    "    space = {'gamma': [0.001, 0.01, 0.1, 1, 10, 100], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    " \n",
    "    grid_search = GridSearchCV(SVR(kernel = 'rbf'), param_grid=space,\n",
    "            cv=KFold(n_splits=10, shuffle=True, random_state=1),\n",
    "        scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    grid_result = grid_search.fit(x_T, y_T)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "#defining the model based on best parameters\n",
    "\n",
    "    svm = SVR(kernel ='rbf', C = grid_search.best_params_['C'],\n",
    "             gamma = grid_search.best_params_['gamma'])\n",
    "    \n",
    "# Perform K-Fold CV\n",
    "\n",
    "    trainscores = cross_val_score(svm, x_T, y_T, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    \n",
    "    return 'bestparameters;', best_params, 'MAE:', trainscores.mean(), svm\n",
    "\n",
    "\n",
    "\n",
    "#'MAE:', -0.7118372187928482\n",
    "#('bestparameters;', {'C': 1, 'gamma': 1}, 'MAE:', -0.7118372187928482) scaled\n",
    "#('bestparameters;', {'C': 100, 'gamma': 1}, 'MAE:', -0.7282753117633405) not scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGED FEATURES\n",
    "\n",
    "print(tune_rbfsvm(XT, y_train.ravel())) # scaled\n",
    "#('bestparameters;', {'C': 1, 'gamma': 0.1}, 'MAE:',-0.56688215 SVR(C=1, gamma=0.1))\n",
    "print(tune_rbfsvm(XT, yT.ravel())) # scaled 2\n",
    "\n",
    "print(tune_rbfsvm(X2_train, y_train.ravel())) # not scaled\n",
    "#'bestparameters;', {'C': 0.001, 'gamma': 0.001}, 'MAE:', MAE:', -0.7278748413, SVR(C=0.001, gamma=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASELINE NOT SCALED TRAINING\n",
    "\n",
    "print(tune_rbfsvm(X2b_train, yb_train.ravel())) # not scaled\n",
    "#('bestparameters;', {'C': 0.1, 'gamma': 0.1}, 'MAE:', -0.5572987216036964, SVR(C=0.1, gamma=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c33b918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bestparameters;', {'C': 0.1, 'gamma': 0.001}, 'MAE:', -0.726604782459138, SVR(C=0.1, gamma=0.001))\n"
     ]
    }
   ],
   "source": [
    "#FOM NOT SCALED TRAINING\n",
    "\n",
    "print(tune_rbfsvm(X2FOM_train, yFOM_train.ravel())) # not scaled\n",
    "#('bestparameters;', {'C': 0.1, 'gamma': 0.001}, 'MAE:', -0.726604782459138, SVR(C=0.1, gamma=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c55364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE: 0.7531155106939699 897\n"
     ]
    }
   ],
   "source": [
    "#Multimodal Errors\n",
    "\n",
    "MSVM_errors = []\n",
    "    \n",
    "svm = SVR(kernel = 'rbf', C = 0.001, gamma = 0.001) # not scaled\n",
    "\n",
    "#scores = cross_val_score(svm, X_test, y_test, cv=10, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "svm.fit(X2_train, y_train.ravel())\n",
    "y_pred = svm.predict(X2_test)\n",
    "    \n",
    "MSVM_errors = abs(y_pred - y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "print(\"\\nMAE:\", mae, len(MSVM_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae905d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b305122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST SCORES NOT SCALED - MULTIMODAL\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error # for calculating the cost function\n",
    "\n",
    "def model_test(X_t, y_t, X_test, y_test):\n",
    "    MSVM_errors = []\n",
    "    \n",
    "    svm = SVR(kernel = 'rbf', C = 0.001, gamma = 0.001) # not scaled\n",
    "\n",
    "    #scores = cross_val_score(svm, X_test, y_test, cv=10, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    svm.fit(X_t, y_t)\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    rmse = float(format(np.sqrt(mean_squared_error(y_test, y_pred)), '.3f'))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return  \"\\nRMSE: \", rmse, '\\nMAE: ', mae, '\\nMSE: ', mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58da40f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE: 0.5574807085533002 2361\n"
     ]
    }
   ],
   "source": [
    "#Baseline Errors\n",
    "\n",
    "BSVM_errors = []\n",
    "\n",
    "svm = SVR(kernel = 'rbf', C = 0.1, gamma = 0.1) # not scaled\n",
    "\n",
    "#scores = cross_val_score(svm, X_test, y_test, cv=10, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "svm.fit(X2b_train, yb_train)\n",
    "\n",
    "y_pred = svm.predict(X2b_test)\n",
    "    \n",
    "mae = mean_absolute_error(yb_test, y_pred)\n",
    "\n",
    "BSVM_errors = abs(y_pred - yb_test)\n",
    "    \n",
    "print(\"\\nMAE:\", mae, len(BSVM_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97b65532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST SCORES NOT SCALED - BASELINE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error # for calculating the cost function\n",
    "\n",
    "def model_test(X_t, y_t, X_test, y_test):\n",
    "    \n",
    "    svm = SVR(kernel = 'rbf', C = 0.1, gamma = 0.1) # not scaled\n",
    "\n",
    "    #scores = cross_val_score(svm, X_test, y_test, cv=10, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    svm.fit(X_t, y_t)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    \n",
    "    \n",
    "    rmse = float(format(np.sqrt(mean_squared_error(y_test, y_pred)), '.3f'))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return  \"\\nRMSE: \", rmse, '\\nMAE: ', mae, '\\nMSE: ', mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7221f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE: 0.7298642002822618 938\n"
     ]
    }
   ],
   "source": [
    "#FOM Errors\n",
    "\n",
    "FOMSVM_errors = []\n",
    "\n",
    "svm = SVR(kernel = 'rbf', C = 0.1, gamma = 0.001) # not scaled\n",
    "    \n",
    "svm.fit(X2FOM_train, yFOM_train)\n",
    "\n",
    "y_pred = svm.predict(X2FOM_test)\n",
    "    \n",
    "mae = mean_absolute_error(yFOM_test, y_pred)\n",
    "\n",
    "FOMSVM_errors = abs(y_pred - yFOM_test)\n",
    "    \n",
    "print(\"\\nMAE:\", mae, len(FOMSVM_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e6848ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST SCORES NOT SCALED - FOM\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error # for calculating the cost function\n",
    "\n",
    "def model_test(X_t, y_t, X_test, y_test):\n",
    "    \n",
    "    svm = SVR(kernel = 'rbf', C = 0.1, gamma = 0.001) # not scaled\n",
    "\n",
    "    #scores = cross_val_score(svm, X_test, y_test, cv=10, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    svm.fit(X_t, y_t)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    \n",
    "    \n",
    "    rmse = float(format(np.sqrt(mean_squared_error(y_test, y_pred)), '.3f'))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return  \"\\nRMSE: \", rmse, '\\nMAE: ', mae, '\\nMSE: ', mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3918ece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nRMSE: ', 0.906, '\\nMAE: ', 0.7531155106939699, '\\nMSE: ', 0.8205068525865323)\n"
     ]
    }
   ],
   "source": [
    "#MERGED FEATURE SET NOT SCALED, TEST SCORES\n",
    "\n",
    "print(model_test(X2_train, y_train.ravel(), X2_test, y_test.ravel()))#not scaled 0.753115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "131adc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nRMSE: ', 0.782, '\\nMAE: ', 0.5574807085533002, '\\nMSE: ', 0.611032123243561)\n"
     ]
    }
   ],
   "source": [
    "#BASELINE FEATURE SET NOT SCALED, TEST SCORES\n",
    "\n",
    "print(model_test(X2b_train, yb_train.ravel(), X2b_test, yb_test.ravel()))#0.557480708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aaa0046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nRMSE: ', 0.9, '\\nMAE: ', 0.7298642002822618, '\\nMSE: ', 0.8104910690074747)\n"
     ]
    }
   ],
   "source": [
    "#FOM FEATURE SET NOT SCALED, TEST SCORES\n",
    "\n",
    "print(model_test(X2FOM_train, yFOM_train.ravel(), X2FOM_test, yFOM_test.ravel()))#0.557480708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eb561e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled, inverse scaled\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error # for calculating the cost function\n",
    "\n",
    "def model_test(X_t, y_t, X_test, y_test):\n",
    "    \n",
    "    svm = SVR(kernel = 'rbf', C = 1, gamma = 0.1) #scaled\n",
    "   #svmNS = SVR(kernel = 'rbf', C = 0.001, gamma = 0.001) #not scaled\n",
    "\n",
    "    #scores = cross_val_score(svm, X_test, y_test, cv=10, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    svm.fit(X_t, y_t)\n",
    "    #y_pred = svm.predict(X_test)\n",
    "    \n",
    "    y_pred = svm.predict(StdS_Xte.transform(X_test)).reshape(-1,1)\n",
    "\n",
    "    \n",
    "    rmse = float(format(np.sqrt(mean_squared_error(y_test, y_pred)), '.3f'))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return  \"\\nRMSE: \", rmse, '\\nMAE: ', mae, '\\nMSE: ', mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "687539ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nRMSE: ', 0.911, '\\nMAE: ', 0.7605642258803795, '\\nMSE: ', 0.8295389475435867)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#MULTIMODAL -TEST RESULTS \n",
    "\n",
    "print(model_test(XT, y_train.ravel(), XTe, y_test.ravel())) #scaled nMAE: ', 0.76056422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f8898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4ffaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8beb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16d287d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef tune_linsvm(x_T, y_T):\\n    \\n    space = {'C': [1, 100]}\\n\\n    \\n    grid_search = GridSearchCV(estimator=SVR(kernel ='linear'), param_grid=space,\\n            cv=KFold(n_splits=5, shuffle=True, random_state=1),\\n        scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\\n    \\n            \\n    grid_result = grid_search.fit(x_T, y_T)\\n    best_params = grid_result.best_params_\\n                                       \\n    svm = SVR(C = grid_search.best_params_['C'])\\n    \\n# Perform K-Fold CV\\n    trainscores = cross_val_score(svm, x_T, y_T, cv=10, scoring='neg_mean_absolute_error')\\n\\n    \\n    return 'bestparameters;', best_params, 'MAE:', trainscores.mean()\\n#('bestparameters;', {'C': 100}, 'MAE:', -0.7985931475954857)  scaled\\n# \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def tune_linsvm(x_T, y_T):\n",
    "    \n",
    "    space = {'C': [1, 100]}\n",
    "\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=SVR(kernel ='linear'), param_grid=space,\n",
    "            cv=KFold(n_splits=5, shuffle=True, random_state=1),\n",
    "        scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "            \n",
    "    grid_result = grid_search.fit(x_T, y_T)\n",
    "    best_params = grid_result.best_params_\n",
    "                                       \n",
    "    svm = SVR(C = grid_search.best_params_['C'])\n",
    "    \n",
    "# Perform K-Fold CV\n",
    "    trainscores = cross_val_score(svm, x_T, y_T, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    \n",
    "    return 'bestparameters;', best_params, 'MAE:', trainscores.mean()\n",
    "#('bestparameters;', {'C': 100}, 'MAE:', -0.7985931475954857)  scaled\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64589aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.svm import SVR\n",
    "\n",
    "def tune_polysvm(x_T, y_T):\n",
    "    \n",
    "    space = {'degree': [2, 5], 'C': [1000, 20000], 'coef0': [0, 1]}\n",
    "                 \n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=SVR(kernel ='poly'), param_grid=space,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=1),\n",
    "        scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "            \n",
    "    grid_result = grid_search.fit(x_T, y_T)\n",
    "    best_params = grid_result.best_params_\n",
    "                                       \n",
    "    svm = SVR(C = grid_search.best_params_['C'])\n",
    "    \n",
    "# Perform K-Fold CV\n",
    "    trainscores = cross_val_score(svm, x_T, y_T, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    \n",
    "    return 'bestparameters;', best_params, 'MAE:', trainscores.mean()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018d7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c5707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c553d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386fd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a627621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATE ATTEMPT OF SVM\n",
    "#https://optunity.readthedocs.io/en/latest/notebooks/notebooks/sklearn-svr.html\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "import sklearn.svm\n",
    "\n",
    "# we explicitly generate the outer_cv decorator so we can use it twice\n",
    "outer_cv = optunity.cross_validated(x=x_train, y=y_train, num_folds=3)\n",
    "\n",
    "def compute_mse_standard(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"Computes MSE of an SVR with RBF kernel and default hyperparameters.\n",
    "    \"\"\"\n",
    "    model = sklearn.svm.SVR().fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    return optunity.metrics.mse(y_test, predictions)\n",
    "\n",
    "# wrap with outer cross-validation\n",
    "compute_mse_standard = outer_cv(compute_mse_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5904987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7758651176070329"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute_mse_standard() returns a three-fold cross-validation estimate of MSE for an SVR \n",
    "#with default hyperparameters.\n",
    "compute_mse_standard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45e6bbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'compute_mse_poly_tuned() returns a three-fold cross-validation estimate of MSE for an \\nSVR with RBF kernel with tuned hyperparameters 1000<C<20000, 2<degree<5 and 0<coef0<1\\nwith a budget of 150 function evaluations. Each tuple of hyperparameters is evaluated using\\ntwice-iterated 5-fold cross-validation.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will create a function that returns MSE based on optimized hyperparameters, where we \n",
    "#choose a polynomial kernel in advance.\n",
    "\n",
    "def compute_mse_poly_tuned(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"Computes MSE of an SVR with RBF kernel and optimized hyperparameters.\"\"\"\n",
    "\n",
    "    # define objective function for tuning\n",
    "    @optunity.cross_validated(x=x_train, y=y_train, num_iter=2, num_folds=5)\n",
    "    def tune_cv(x_train, y_train, x_test, y_test, C, degree, coef0):\n",
    "        model = sklearn.svm.SVR(C=C, degree=degree, coef0=coef0, kernel='poly').fit(x_train, y_train)\n",
    "        predictions = model.predict(x_test)\n",
    "        return optunity.metrics.mse(y_test, predictions)\n",
    "\n",
    "    # optimize parameters\n",
    "    optimal_pars, _, _ = optunity.minimize(tune_cv, 150, C=[1000, 20000], degree=[2, 5], coef0=[0, 1])\n",
    "    print(\"optimal hyperparameters: \" + str(optimal_pars))\n",
    "\n",
    "    tuned_model = sklearn.svm.SVR(kernel='poly', **optimal_pars).fit(x_train, y_train)\n",
    "    predictions = tuned_model.predict(x_test)\n",
    "    return optunity.metrics.mse(y_test, predictions)\n",
    "\n",
    "# wrap with outer cross-validation\n",
    "compute_mse_poly_tuned = outer_cv(compute_mse_poly_tuned)\n",
    "\n",
    "'''compute_mse_poly_tuned() returns a three-fold cross-validation estimate of MSE for an \n",
    "SVR with RBF kernel with tuned hyperparameters 1000<C<20000, 2<degree<5 and 0<coef0<1\n",
    "with a budget of 150 function evaluations. Each tuple of hyperparameters is evaluated using\n",
    "twice-iterated 5-fold cross-validation.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dccb5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mse_poly_tuned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4c739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96e71e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The polynomial kernel yields pretty good results when optimized, but maybe we can do even \n",
    "#better with an RBF kernel.\n",
    "\n",
    "\n",
    "def compute_mse_rbf_tuned(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"Computes MSE of an SVR with RBF kernel and optimized hyperparameters.\"\"\"\n",
    "\n",
    "    # define objective function for tuning\n",
    "    @optunity.cross_validated(x=x_train, y=y_train, num_iter=2, num_folds=5)\n",
    "    def tune_cv(x_train, y_train, x_test, y_test, C, gamma):\n",
    "        model = sklearn.svm.SVR(C=C, gamma=gamma).fit(x_train, y_train)\n",
    "        predictions = model.predict(x_test)\n",
    "        return optunity.metrics.mse(y_test, predictions)\n",
    "\n",
    "    # optimize parameters\n",
    "    optimal_pars, _, _ = optunity.minimize(tune_cv, 150, C=[1, 100], gamma=[0, 50])\n",
    "    print(\"optimal hyperparameters: \" + str(optimal_pars))\n",
    "\n",
    "    tuned_model = sklearn.svm.SVR(**optimal_pars).fit(x_train, y_train)\n",
    "    predictions = tuned_model.predict(x_test)\n",
    "    return optunity.metrics.mse(y_test, predictions)\n",
    "\n",
    "# wrap with outer cross-validation\n",
    "compute_mse_rbf_tuned = outer_cv(compute_mse_rbf_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3678610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal hyperparameters: {'C': 42.464780148239306, 'gamma': 0.01732551601730581}\n",
      "optimal hyperparameters: {'C': 84.19416015624999, 'gamma': 1.3927476169293715}\n",
      "optimal hyperparameters: {'C': 4.656571032577705, 'gamma': 1.191587233088118}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8686365147179491"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''compute_mse_rbf_tuned() returns a three-fold cross-validation estimate of MSE for an SVR\n",
    "with RBF kernel with tuned hyperparameters 1<C<100 and 0<γ<5 with a budget of 150 function \n",
    "evaluations. Each tuple of hyperparameters is evaluated using twice-iterated 5-fold \n",
    "cross-validation.'''\n",
    "\n",
    "compute_mse_rbf_tuned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f41613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optunity can optimize conditional search spaces, here the kernel family and depending on\n",
    "which family the hyperparameterization (γ, degree, coef0, ...). We start by defining the \n",
    "search space (we will try the linear, polynomial and RBF kernel).'''\n",
    "\n",
    "space = {'kernel': {'linear': {'C': [0, 100]},\n",
    "                    'rbf': {'gamma': [0, 50], 'C': [1, 100]},\n",
    "                    'poly': {'degree': [2, 5], 'C': [1000, 20000], 'coef0': [0, 1]}\n",
    "                    }\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "900ae03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_all_tuned(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"Computes MSE of an SVR with RBF kernel and optimized hyperparameters.\"\"\"\n",
    "\n",
    "    # define objective function for tuning\n",
    "    @optunity.cross_validated(x=x_train, y=y_train, num_iter=2, num_folds=5)\n",
    "    def tune_cv(x_train, y_train, x_test, y_test, kernel, C, gamma, degree, coef0):\n",
    "        if kernel == 'linear':\n",
    "            model = sklearn.svm.SVR(kernel=kernel, C=C)\n",
    "        elif kernel == 'poly':\n",
    "            model = sklearn.svm.SVR(kernel=kernel, C=C, degree=degree, coef0=coef0)\n",
    "        elif kernel == 'rbf':\n",
    "            model = sklearn.svm.SVR(kernel=kernel, C=C, gamma=gamma)\n",
    "        else:\n",
    "            raise ArgumentError(\"Unknown kernel function: %s\" % kernel)\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        predictions = model.predict(x_test)\n",
    "        return optunity.metrics.mse(y_test, predictions)\n",
    "\n",
    "    # optimize parameters\n",
    "    optimal_pars, _, _ = optunity.minimize_structured(tune_cv, num_evals=150, search_space=space)\n",
    "\n",
    "    # remove hyperparameters with None value from optimal pars\n",
    "    for k, v in optimal_pars.items():\n",
    "        if v is None: del optimal_pars[k]\n",
    "    print(\"optimal hyperparameters: \" + str(optimal_pars))\n",
    "\n",
    "    tuned_model = sklearn.svm.SVR(**optimal_pars).fit(x_train, y_train)\n",
    "    predictions = tuned_model.predict(x_test)\n",
    "    return optunity.metrics.mse(y_test, predictions)\n",
    "\n",
    "# wrap with outer cross-validation\n",
    "compute_mse_all_tuned = outer_cv(compute_mse_all_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01359cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now the kernel family will be optimized along with its hyperparameterization.\n",
    "\n",
    "compute_mse_all_tuned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d4260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f2385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9630ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 938\n",
      "2361 2361\n",
      "897 897\n"
     ]
    }
   ],
   "source": [
    "# SIGNIFICANCE OF DIFFERENCE\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "print(len(FOMSVM_errors), len(FOMbaseline_errors))\n",
    "print(len(BSVM_errors), len(Bbaseline_errors))\n",
    "print(len(MSVM_errors), len(Mbaseline_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7abbfeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WilcoxonResult(statistic=179246.5, pvalue=0.004353793906067737)\n",
      "WilcoxonResult(statistic=210977.0, pvalue=0.2666944649413523)\n",
      "WilcoxonResult(statistic=758678.5, pvalue=5.1166597855801505e-82)\n"
     ]
    }
   ],
   "source": [
    "#conduct the Wilcoxon-Signed Rank Test: SVM vs Baseline\n",
    "\n",
    "Mdif = stats.wilcoxon(MSVM_errors, Mbaseline_errors)\n",
    "FOMdif = stats.wilcoxon(FOMSVM_errors, FOMbaseline_errors)\n",
    "Bdif = stats.wilcoxon(BSVM_errors, Bbaseline_errors)\n",
    "\n",
    "print(Mdif)\n",
    "print(FOMdif)\n",
    "print(Bdif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf2d4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000000'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:.8f}\".format(float(\"5.1166597855801505e-82\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92186a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(938, 1) (938,) <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "MRFERRORS = pd.read_csv('/Users/selinzobu/Desktop/TILES/1MRFERRORS.csv')\n",
    "MRFERRORS = MRFERRORS.loc[:, ~MRFERRORS.columns.str.contains('^Unnamed')]\n",
    "\n",
    "#MRFERRORS = MRFERRORS.values.tolist()\n",
    "\n",
    "BRFERRORS = pd.read_csv('/Users/selinzobu/Desktop/TILES/1BRFERRORS.csv')\n",
    "BRFERRORS = BRFERRORS.loc[:, ~BRFERRORS.columns.str.contains('^Unnamed')]\n",
    "\n",
    "#BRFERRORS = BRFERRORS.values.tolist()\n",
    "\n",
    "FOMRFERRORS = pd.read_csv('/Users/selinzobu/Desktop/TILES/1FOMRFERRORS.csv')\n",
    "FOMRFERRORS = FOMRFERRORS.loc[:, ~FOMRFERRORS.columns.str.contains('^Unnamed')]\n",
    "\n",
    "#FOMRFERRORS = FOMRFERRORS.values.tolist()\n",
    "print(FOMRFERRORS.shape, FOMRFERRORS.squeeze().shape, type(FOMRFERRORS.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99fd575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(MSVM_errors), type(MRFERRORS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cb0fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WilcoxonResult(statistic=120789.0, pvalue=2.975169608234801e-25)\n",
      "WilcoxonResult(statistic=173642.0, pvalue=2.034053896483224e-08)\n",
      "WilcoxonResult(statistic=1166390.0, pvalue=6.162215274115193e-12)\n"
     ]
    }
   ],
   "source": [
    "#conduct the Wilcoxon-Signed Rank Test : RF vs SVM\n",
    "\n",
    "Mdif = stats.wilcoxon( MRFERRORS.squeeze(), MSVM_errors)\n",
    "FOMdif = stats.wilcoxon( FOMRFERRORS.squeeze(), FOMSVM_errors)\n",
    "Bdif = stats.wilcoxon(BRFERRORS.squeeze(), BSVM_errors)\n",
    "\n",
    "print(Mdif)\n",
    "print(FOMdif)\n",
    "print(Bdif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f6f7f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WilcoxonResult(statistic=120789.0, pvalue=2.975169608234801e-25)\\nWilcoxonResult(statistic=173642.0, pvalue=2.034053896483224e-08)\\nWilcoxonResult(statistic=1166390.0, pvalue=6.162215274115193e-12)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''WilcoxonResult(statistic=120789.0, pvalue=2.975169608234801e-25)\n",
    "WilcoxonResult(statistic=173642.0, pvalue=2.034053896483224e-08)\n",
    "WilcoxonResult(statistic=1166390.0, pvalue=6.162215274115193e-12)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "328a4ced",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The samples x and y must have the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/1sxcpd491kg32hysvmfw5d3r0000gn/T/ipykernel_1044/2852964987.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#conduct the Wilcoxon-Signed Rank Test : MERGED VS SURVEY ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mMBRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwilcoxon\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mMRFERRORS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBRFERRORS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# RF: M vs B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mMFOMRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwilcoxon\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mMRFERRORS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mFOMMRFERRORS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#RF: M vs FOM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/scipy/stats/morestats.py\u001b[0m in \u001b[0;36mwilcoxon\u001b[0;34m(x, y, zero_method, correction, alternative, mode)\u001b[0m\n\u001b[1;32m   3127\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Samples x and y must be one-dimensional.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The samples x and y must have the same length.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3130\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The samples x and y must have the same length."
     ]
    }
   ],
   "source": [
    "#conduct the Wilcoxon-Signed Rank Test : MERGED VS SURVEY ONLY\n",
    "\n",
    "MBRF = stats.wilcoxon( MRFERRORS.squeeze(), BRFERRORS.squeeze()) # RF: M vs B\n",
    "MFOMRF = stats.wilcoxon(  MRFERRORS.squeeze(),  FOMMRFERRORS.squeeze()) #RF: M vs FOM\n",
    "\n",
    "\n",
    "MBSVM = stats.wilcoxon(MSVM_errors, BSVM_errors) # SVM : M vs B\n",
    "MFOMSVM = stats.wilcoxon(MSVM_errors, FOMSVM_errors) # SVM: M vs FOM\n",
    "\n",
    "\n",
    "MBBASE = stats.wilcoxon(Mbaseline_errors, Bbaseline_errors) # BASE: M vs B\n",
    "MFOMBASE = stats.wilcoxon(  Mbaseline_errors,  FOMbaseline_errors) #BASE: M vs FOM\n",
    "\n",
    "\n",
    "print(MBRF)\n",
    "print(MFOMRF)\n",
    "print(MBSVM)\n",
    "print(MFOMSVM)\n",
    "print(MBBASE)\n",
    "print(MFOMBASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa925a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fe9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77145788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
